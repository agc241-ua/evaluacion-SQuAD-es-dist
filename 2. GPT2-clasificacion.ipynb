{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN0NR58wgKNcC2+w3bI2L8u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#MODEL TEST"],"metadata":{"id":"Jh13y4HBW3VU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-h1VDXaFWxFc"},"outputs":[],"source":["import json\n","import torch\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n","\n","import time\n","import random\n","import numpy as np"]},{"cell_type":"code","source":["#Debemos fijar el parámetro de random seed siempre al mismo valor y después cargar el modelo para que el modelo devuelva siempre el mismo valor\n","# SET RANDOM SEED\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","# Cargamos los modelo y el tokenizer\n","#GPT2-SMALL-SPANISH\n","gpt2_spanish= \"datificate/gpt2-small-spanish\"\n","tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(gpt2_spanish)\n","tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token #Necesario porque gpt2 no tiene pad_token, por lo que asignamos eos_token\n","\n","\n","def define_labels(file):\n","  return max(len(item[\"choices\"]) for item in json.load(open(file))[\"data\"])\n","max_labels = define_labels(\"1-test.json\")\n","print(f\"El máximo número de opciones es: {max_labels}\")\n","\n","\n","#cargamos el modelo con el num_labels\n","modelGPT2 = GPT2ForSequenceClassification.from_pretrained(gpt2_spanish, num_labels=max_labels)  #labels = opciones de respuesta\n","model_train = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=max_labels)\n","model_train.config.pad_token_id = model_train.config.eos_token_id\n"],"metadata":{"id":"itCthw6xXE5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"1-test.json\", \"r\", encoding=\"utf-8\") as f:\n","    test_dataset = json.load(f)[\"data\"]\n","\n","OPTION_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n","def format_prompt(context, question, choices):\n","  prompt = f\"Dado el siguiente contexto:\\n{context}\\nPregunta: {question}\\nOpciones:\\n\" + \\\n","         \"\\n\".join([f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)]) + \\\n","         \"\\nRespuesta correcta:\"\n","  return prompt"],"metadata":{"id":"sC9WRWKHXG9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","def predict_answerGPT2(inputs, choices, num_choices, temperature = 1.0):\n","  with torch.no_grad():\n","    outputs = modelGPT2(**inputs)\n","\n","    logits = outputs.logits.squeeze(0)[:num_choices]  #quitamos opción extra, si la hay\n","    sacled_logits = logits / temperature\n","    probs = F.softmax(sacled_logits, dim=-1)\n","\n","    predicted_index = torch.argmax(probs).item()\n","    if predicted_index >= num_choices:\n","      print(f\"Error: Índice de predicción {predicted_index} fuera de rango para {num_choices} opciones. Saltamos la pregunta.\")\n","      return None\n","\n","    predicted_answer = choices[predicted_index]\n","  return predicted_answer\n"],"metadata":{"id":"62KQC0qNik8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct_predictionsGPT2 = 0\n","total_questions = len(test_dataset)\n","predictionsGPT2 = []\n","# print(f\"Total de preguntas: {total_questions}\")\n","\n","start_time = time.time()\n","for item in test_dataset:\n","  is_correct_prediction = False\n","  context = item[\"context\"]\n","  question = item[\"question\"]\n","  choices = [choice[\"text\"] for choice in item[\"choices\"]]\n","  correct_answer = next(choice[\"text\"] for choice in item[\"choices\"] if choice[\"type\"] == \"correct answer\")\n","\n","  num_choices = len(choices)\n","  if num_choices > max_labels:\n","    print(f\"Error: Pregunta con {num_choices} opciones. Supera el máximo permitido: {max_labels}\")\n","    continue\n","\n","  prompt = format_prompt(context, question, choices)\n","  inputsGPT2 = tokenizer_gpt2(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","\n","  predicted_answerGPT2 = predict_answerGPT2(inputsGPT2, choices, num_choices)\n","\n","\n","  if correct_answer == predicted_answerGPT2:\n","    correct_predictionsGPT2 += 1\n","    is_correct_prediction = True\n","\n","\n","  predictionsGPT2.append({\n","    \"context\": context,\n","    \"question\": question,\n","    \"choices\": choices,\n","    \"correct_answer\": correct_answer,\n","    \"predicted_answer\": predicted_answerGPT2,\n","    \"is_correct\": is_correct_prediction\n","  })\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","print()\n","\n","with open(\"predictionsGPT2.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(predictionsGPT2, f, indent=4, ensure_ascii=False)\n","\n","\n","print(f\"Total de preguntas evaluadas: {total_questions}\")\n","print(f\"Respuestas correctas GPT2: {correct_predictionsGPT2}\")\n","print(f\"Precisión del modelo GPT2: {correct_predictionsGPT2 / total_questions:.2%}\")\n","print()\n","# df = pd.DataFrame(predictionsBERT)\n","# print(df)"],"metadata":{"id":"vlM5RSLQYetf"},"execution_count":null,"outputs":[]}]}