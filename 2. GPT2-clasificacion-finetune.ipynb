{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOkbdU5T4neE9psCF1ajADr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_3tal-QF_arQ"},"outputs":[],"source":["import json\n","import torch\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n","!pip install datasets\n","from datasets import Dataset\n","import time\n","import random\n","import numpy as np"]},{"cell_type":"code","source":["#Debemos fijar el parámetro de random seed siempre al mismo valor y después cargar el modelo para que el modelo devuelva siempre el mismo valor\n","# SET RANDOM SEED\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n"],"metadata":{"id":"-PClHDtmpCus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ENTRENAMIENTO"],"metadata":{"id":"l66HOSCupPsH"}},{"cell_type":"code","source":["OPTION_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n","\n","# Cargar el dataset de entrenamiento o test\n","def load_dataset(file_path):\n","  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)[\"data\"]\n","\n","  # Convertir el JSON a un formato adecuado\n","  formatted_data = []\n","  max_labels = 0\n","  for item in data:\n","      context = item[\"context\"]\n","      question = item[\"question\"]\n","      choices = [choice[\"text\"] for choice in item[\"choices\"]]\n","      correct_answer = next(i for i, choice in enumerate(item[\"choices\"]) if choice[\"type\"] == \"correct answer\")  #El número!!\n","      prompt = f\"Dado el siguiente contexto:\\n{context}\\nPregunta: {question}\\nOpciones:\\n\" + \\\n","                \"\\n\".join([f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)]) + \\\n","                \"\\nSelecciona la respuesta correcta:\"\n","      formatted_data.append({\"text\": prompt, \"label\": correct_answer})\n","      max_labels = max(max_labels, len(choices))\n","  return formatted_data, max_labels"],"metadata":{"id":"K8bEJoQ83PdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargar datos\n","train_data, max_labels_train = load_dataset(\"1-training.json\")\n","dev_data, max_labels_dev = load_dataset(\"1-dev.json\")\n","num_labels = max_labels_train\n","\n","# Cargar tokenizer y modelo\n","model_name = \"datificate/gpt2-small-spanish\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token  # GPT-2 no tiene pad token\n","model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","model.config.pad_token_id = model.config.eos_token_id\n"],"metadata":{"id":"uliBTYqP4A7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ClassificationDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        encoding = tokenizer(item[\"text\"], padding=\"max_length\", truncation=True,\n","                             max_length=self.max_length, return_tensors=\"pt\")\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": torch.tensor(item[\"label\"])\n","        }\n","\n","train_dataset = ClassificationDataset(train_data, tokenizer)\n","dev_dataset = ClassificationDataset(dev_data, tokenizer)\n"],"metadata":{"id":"bQkS_6Ld4THu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","import time\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","start_time = time.time()\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-results\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    weight_decay=0.1,\n","    logging_dir=\"./gpt2-logs\",\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    fp16=True,\n","    report_to=\"none\"\n",")\n","\n","\n","\n","trainer_gpt2 = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=dev_dataset,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",")\n","\n","\n","trainer_gpt2.train()\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","\n","# Mostrar tiempo en minutos y segundos\n","mins, secs = divmod(elapsed_time, 60)\n","print(f\"Tiempo total de entrenamiento: {int(mins)} min {int(secs)} sec\")\n","\n","\n","trainer_gpt2.save_model(\"modelo_finetuned\")\n","tokenizer.save_pretrained(\"modelo_finetuned\")"],"metadata":{"id":"q1XrAAGdqS21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EVALUACIÓN"],"metadata":{"id":"AjnEYwXqpSQv"}},{"cell_type":"code","source":["modelo_finetuned = \"modelo_finetuned\"\n","tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(modelo_finetuned)\n","modelGPT2 = GPT2ForSequenceClassification.from_pretrained(modelo_finetuned)\n","\n","with open(\"4-test.json\", \"r\", encoding=\"utf-8\") as f:\n","    test_dataset = json.load(f)[\"data\"]\n","\n","def format_prompt(context, question, choices):\n","  prompt = f\"Dado el siguiente contexto:\\n{context}\\nPregunta: {question}\\nOpciones:\\n\" + \\\n","         \"\\n\".join([f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)]) + \\\n","         \"\\nRespuesta correcta:\"\n","  return prompt\n","\n","import torch.nn.functional as F\n","def predict_answerGPT2(inputs, choices, num_choices, temperature = 1.0):\n","  with torch.no_grad():\n","    outputs = modelGPT2(**inputs)\n","\n","    logits = outputs.logits.squeeze(0)[:num_choices]  #quitamos opción extra, si la hay\n","    sacled_logits = logits / temperature\n","    probs = F.softmax(sacled_logits, dim=-1)\n","\n","    predicted_index = torch.argmax(probs).item()\n","    if predicted_index >= num_choices:\n","      print(f\"Error: Índice de predicción {predicted_index} fuera de rango para {num_choices} opciones. Saltamos la pregunta.\")\n","      return None\n","\n","    predicted_answer = choices[predicted_index]\n","  return predicted_answer\n"],"metadata":{"id":"1KbXxrpXpUEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct_predictionsGPT2 = 0\n","total_questions = len(test_dataset)\n","predictionsGPT2 = []\n","# print(f\"Total de preguntas: {total_questions}\")\n","\n","start_time = time.time()\n","for item in test_dataset:\n","  is_correct_prediction = False\n","  context = item[\"context\"]\n","  question = item[\"question\"]\n","  choices = [choice[\"text\"] for choice in item[\"choices\"]]\n","  correct_answer = next(choice[\"text\"] for choice in item[\"choices\"] if choice[\"type\"] == \"correct answer\")\n","\n","  num_choices = len(choices)\n","  if num_choices > num_labels:\n","    print(f\"Error: Pregunta con {num_choices} opciones. Supera el máximo permitido: {max_labels}\")\n","    continue\n","\n","  prompt = format_prompt(context, question, choices)\n","  inputsGPT2 = tokenizer_gpt2(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","\n","  predicted_answerGPT2 = predict_answerGPT2(inputsGPT2, choices, num_choices)\n","\n","\n","  if correct_answer == predicted_answerGPT2:\n","    correct_predictionsGPT2 += 1\n","    is_correct_prediction = True\n","\n","\n","  predictionsGPT2.append({\n","    \"context\": context,\n","    \"question\": question,\n","    \"choices\": choices,\n","    \"correct_answer\": correct_answer,\n","    \"predicted_answer\": predicted_answerGPT2,\n","    \"is_correct\": is_correct_prediction\n","  })\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","print()\n","\n","with open(\"predictionsGPT2.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(predictionsGPT2, f, indent=4, ensure_ascii=False)\n","\n","\n","print(f\"Total de preguntas evaluadas: {total_questions}\")\n","print(f\"Respuestas correctas GPT2: {correct_predictionsGPT2}\")\n","print(f\"Precisión del modelo GPT2: {correct_predictionsGPT2 / total_questions:.2%}\")\n","print()\n"],"metadata":{"id":"cs9ShQqlpV-1"},"execution_count":null,"outputs":[]}]}