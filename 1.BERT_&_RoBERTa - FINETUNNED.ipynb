{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPLm4VIMczAZD08om844kcm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"55EaeIkCXafV"},"outputs":[],"source":["import json\n","import torch\n","import torch.nn.functional as F\n","import time\n","import random\n","import numpy as np\n","!pip install --upgrade transformers\n","import transformers\n","# from transformers import RobertaTokenizer, RobertaForMultipleChoice\n","from transformers import BertTokenizer, BertForMultipleChoice\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from torch.utils.data import Dataset\n"]},{"cell_type":"markdown","source":["#ENTRENO"],"metadata":{"id":"etO8sGEfsc55"}},{"cell_type":"code","source":["# SET RANDOM SEED\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Modelo y tokenizer\n","bert = \"google-bert/bert-base-multilingual-cased\"\n","tokenizer = BertTokenizer.from_pretrained(bert)\n","model = BertForMultipleChoice.from_pretrained(bert)\n","\n","# roberta = \"PlanTL-GOB-ES/roberta-base-bne\"\n","# tokenizer = RobertaTokenizer.from_pretrained(roberta)\n","# model = RobertaForMultipleChoice.from_pretrained(roberta)"],"metadata":{"id":"Q2UrbdXbZ0Ky","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Personalizado"],"metadata":{"id":"INI2FYPktxz9"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class MultipleChoiceDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        context = item[\"context\"]\n","        question = item[\"question\"]\n","        choices = [c[\"text\"] for c in item[\"choices\"]]\n","        label = [i for i, c in enumerate(item[\"choices\"]) if c[\"type\"] == \"correct answer\"][0]\n","        texts = [f\"{context} Pregunta: {question} Opción: {choice}\" for choice in choices]\n","\n","        encodings = self.tokenizer(\n","            texts,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","\n","        max_num_choices = max(len(d['choices']) for d in self.data)\n","\n","        padding_shape = (max_num_choices - encodings['input_ids'].shape[0], self.max_length)\n","        encodings['input_ids'] = torch.cat([encodings['input_ids'], torch.zeros(padding_shape, dtype=encodings['input_ids'].dtype)], dim=0)\n","        encodings['attention_mask'] = torch.cat([encodings['attention_mask'], torch.zeros(padding_shape, dtype=encodings['attention_mask'].dtype)], dim=0)\n","\n","        return {\n","            \"input_ids\": encodings[\"input_ids\"],\n","            \"attention_mask\": encodings[\"attention_mask\"],\n","            \"labels\": label\n","        }"],"metadata":{"id":"opcAQr4WsYNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cargamos los subconjuntos training y dev\n"],"metadata":{"id":"t3OIKIgssbcg"}},{"cell_type":"code","source":["with open(\"1-training.json\", \"r\", encoding=\"utf-8\") as f:\n","    train_data = json.load(f)[\"data\"]\n","\n","with open(\"1-dev.json\", \"r\", encoding=\"utf-8\") as f:\n","    dev_data = json.load(f)[\"data\"]\n","\n","train_dataset = MultipleChoiceDataset(train_data, tokenizer)\n","dev_dataset = MultipleChoiceDataset(dev_data, tokenizer)\n","\n","print(TrainingArguments.__module__)"],"metadata":{"id":"nVPzM-heuPgx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamos el modelo:"],"metadata":{"id":"Lzp_hpiPubDN"}},{"cell_type":"code","source":["!pip install --upgrade transformers\n","import transformers\n","import time\n","print(transformers.__version__)\n","from transformers import Trainer, TrainingArguments\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","start_time = time.time()\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    report_to=\"none\",\n","    fp16=True\n",")\n","\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=dev_dataset,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",")\n","\n","trainer.train()\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","trainer.save_model(\"modelo_finetuned\")\n","tokenizer.save_pretrained(\"modelo_finetuned\")"],"metadata":{"id":"jfoQl54kue8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EVALUACIÓN"],"metadata":{"id":"koM8hJu5sY2P"}},{"cell_type":"code","source":["# Cargar dataset\n","with open(\"4-test-BERT.json\", \"r\", encoding=\"utf-8\") as f:\n","    test_dataset = json.load(f)[\"data\"]\n","\n","def format_pair(context, question, choice):\n","    return f\"{context} Pregunta: {question} Opción: {choice}\"\n","\n","def prepare_inputs(context, question, choices):\n","    paired_texts = [format_pair(context, question, choice) for choice in choices]\n","    #Tokenizamos el prompt --> se pasa a numeros\n","    encoded = tokenizer(paired_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n","    return {\n","        #IDs de los tokens del modelo\n","        'input_ids': encoded['input_ids'].unsqueeze(0).to(model.device),\n","        #máscara que indica qué tokens son reales y cuales son padding\n","        'attention_mask': encoded['attention_mask'].unsqueeze(0).to(model.device),\n","    }\n","\n","#El modelo predice la opción correcta\n","def predict_answer(inputs, choices):\n","#torch.no_grad --> más rápido y consume menos memoria\n","  with torch.no_grad():\n","    #\n","    outputs = model(**inputs)\n","    logits = outputs.logits.squeeze(0)\n","\n","    probs = F.softmax(logits, dim=-1)\n","\n","    predicted_index = torch.argmax(probs).item()\n","\n","\n","    if predicted_index >= len(choices):\n","        print(f\"Error: Índice de predicción {predicted_index} fuera de rango.\")\n","        return None\n","\n","    return choices[predicted_index]\n"],"metadata":{"id":"eZq9jQCEayfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_labels = max(len(item[\"choices\"]) for item in json.load(open(\"4-test-BERT.json\"))[\"data\"])\n","# max_labels = max(len(item[\"choices\"]) for item in json.load(open(\"1-test.json\"))[\"data\"])\n","correct_predictionsBERT = 0\n","total_questions = len(test_dataset)\n","predictions = []\n","\n","start_time = time.time()\n","for item in test_dataset:\n","  is_correct_prediction = False\n","  context = item[\"context\"]\n","  question = item[\"question\"]\n","  choices = [choice[\"text\"] for choice in item[\"choices\"]]\n","  correct_answer = next(choice[\"text\"] for choice in item[\"choices\"] if choice[\"type\"] == \"correct answer\")\n","\n","  num_choices = len(choices)\n","  if num_choices > max_labels:\n","    print(f\"Error: Pregunta con {num_choices} opciones. Supera el máximo permitido: {max_labels}\")\n","    continue\n","  if num_choices == 0:\n","    print(f\"Pregunta con {num_choices} opciones.\")\n","    continue\n","\n","  # prompt = format_prompt(context, question, choices)\n","  # inputsBERT = tokenizer_bert(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","  # predicted_answerBERT = predict_answerBERT(inputsBERT, choices, num_choices)\n","  inputs = prepare_inputs(context, question, choices)\n","  predicted_answer = predict_answer(inputs, choices)\n","\n","\n","  if correct_answer == predicted_answer:\n","    correct_predictionsBERT += 1\n","    is_correct_prediction = True\n","\n","\n","  predictions.append({\n","    \"context\": context,\n","    \"question\": question,\n","    \"choices\": choices,\n","    \"correct_answer\": correct_answer,\n","    \"predicted_answer\": predicted_answer,\n","    \"is_correct\": is_correct_prediction\n","  })\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","\n","with open(\"predictionsBERT-opt.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(predictions, f, indent=4, ensure_ascii=False)\n","\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","print()\n","print(f\"Total de preguntas evaluadas: {total_questions}\")\n","print(f\"Respuestas correctas BERT: {correct_predictionsBERT}\")\n","print(f\"Precisión del modelo BERT: {correct_predictionsBERT / total_questions:.2%}\")\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecuKc9xRc4JT","executionInfo":{"status":"ok","timestamp":1747737787177,"user_tz":-120,"elapsed":2078,"user":{"displayName":"Alba García Cáceres","userId":"16957210619186256406"}},"outputId":"d56a449f-aead-4dde-b220-e22ae2a0108c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiempo de ejecución: 2.06 segundos\n","\n","Total de preguntas evaluadas: 50\n","Respuestas correctas BERT: 28\n","Precisión del modelo BERT: 56.00%\n","\n"]}]}]}