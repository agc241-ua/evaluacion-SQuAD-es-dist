{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcz33SV3OZCn"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qicM96UNbnvf"
   },
   "outputs": [],
   "source": [
    "#Debemos fijar el parámetro de random seed siempre al mismo valor y después cargar el modelo para que el modelo devuelva siempre el mismo valor\n",
    "# SET RANDOM SEED\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "#model_id = \"BSC-LT/salamandra-2b-instruct\"\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_id = \"datificate/gpt2-small-spanish\"\n",
    "# model_id = \"BSC-LT/Flor-6.3B-Instruct-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model= AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\")\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ02SrEwb1lt"
   },
   "source": [
    "#ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xldrvKab1B_"
   },
   "outputs": [],
   "source": [
    "OPTION_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "def convert_to_chat_format(input_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    chat_formatted = []\n",
    "    for item in data:\n",
    "        try:\n",
    "            context = item[\"context\"]\n",
    "            question = item[\"question\"]\n",
    "            choices = [choice[\"text\"] for choice in item[\"choices\"]]\n",
    "            correct_index = next(i for i, choice in enumerate(item[\"choices\"]) if choice[\"type\"] == \"correct answer\")\n",
    "            correct_text = choices[correct_index]\n",
    "            correct_letter = OPTION_LETTERS[correct_index]\n",
    "\n",
    "            opciones = \"\\n\".join([f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)])\n",
    "            input_text = (\n",
    "                f\"Dado el siguiente contexto:\\n{context}\\n\\n\"\n",
    "                f\"Pregunta: {question}\\n\\n\"\n",
    "                f\"Opciones:\\n{opciones}\\n\\n\"\n",
    "                f\"Selecciona la respuesta correcta:\"\n",
    "            )\n",
    "\n",
    "            chat_formatted.append({\n",
    "                \"system\": \"Eres un asistente de inteligencia artificial especializado en responder a preguntas de opción múltiple. Tu tarea es responder correctamente a las preguntas que se te planteen.\",\n",
    "                \"input\": input_text,\n",
    "                \"target\": correct_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error en una entrada: {e}\")\n",
    "            continue\n",
    "    return chat_formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubayyk_cb5wq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CausalLMDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      system = self.data[idx][\"system\"]\n",
    "      input = self.data[idx][\"input\"]\n",
    "      target = self.data[idx][\"target\"]\n",
    "        # prompt = self.data[idx][\"prompt\"]\n",
    "        # completion = self.data[idx][\"completion\"]\n",
    "      full_text = system + \" \" + input + \" \" + target\n",
    "      # full_text = prompt + \" \" + completion\n",
    "      enc = self.tokenizer(full_text, truncation=True, padding=\"max_length\",\n",
    "                            max_length=self.max_length, return_tensors=\"pt\")\n",
    "      enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "      enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
    "      return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-6O2Zoxb7JU"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "train_data = convert_to_chat_format(\"1-training.json\")\n",
    "dev_data = convert_to_chat_format(\"1-dev.json\")\n",
    "\n",
    "train_dataset = CausalLMDataset(train_data, tokenizer)\n",
    "dev_dataset = CausalLMDataset(dev_data, tokenizer)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Mostrar tiempo en minutos y segundos\n",
    "mins, secs = divmod(elapsed_time, 60)\n",
    "print(f\"Tiempo total de entrenamiento: {int(mins)} min {int(secs)} sec\")\n",
    "\n",
    "\n",
    "trainer.save_model(\"modelo_finetuned\")\n",
    "tokenizer.save_pretrained(\"modelo_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lbSENyKcAqh"
   },
   "source": [
    "#EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ivDnG0ccA4J"
   },
   "outputs": [],
   "source": [
    "OPTION_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "modelo_finetuned = \"modelo_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_finetuned)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelo_finetuned,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Carga del dataset\n",
    "with open(\"1-test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_dataset = json.load(f)[\"data\"]\n",
    "# with open(\"4-test-LLAMA1B-1024.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_dataset = json.load(f)[\"data\"]\n",
    "\n",
    "# Letras para las opciones\n",
    "OPTION_LETTERS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "def prompt_format(context, question, choices):\n",
    "  prompt = f\"Dado el siguiente contexto:\\n{context}\\nPregunta: {question}\\nOpciones:\\n\" + \\\n",
    "         \"\\n\".join([f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)]) + \\\n",
    "         \"\\nRespuesta correcta:\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "\n",
    "# Evaluación\n",
    "correct_predictions = 0\n",
    "total_questions = len(test_dataset)\n",
    "predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for item in test_dataset:\n",
    "    context = item[\"context\"]\n",
    "    question = item[\"question\"]\n",
    "    choices = [choice[\"text\"] for choice in item[\"choices\"]]\n",
    "    correct_answer = next(choice[\"text\"] for choice in item[\"choices\"] if choice[\"type\"] == \"correct answer\")\n",
    "\n",
    "    if len(choices) > len(OPTION_LETTERS):\n",
    "        print(f\"Demasiadas opciones para la pregunta: '{question}'\")\n",
    "        continue\n",
    "\n",
    "    prompt = prompt_format(context, question, choices)\n",
    "    inputs_ids = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(model.device)\n",
    "\n",
    "    # Estimar longitud media de las opciones (en palabras)\n",
    "    avg_words_per_option = np.mean([len(option.split()) for option in choices])\n",
    "    estimated_tokens_needed = int(avg_words_per_option * 1.5) + 10  # 1.5x palabras ≈ tokens, +5 de margen\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs_ids,\n",
    "                                      max_new_tokens=estimated_tokens_needed,\n",
    "                                      do_sample=False,\n",
    "                                      pad_token_id=tokenizer.pad_token_id)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    respuesta_generada = generated_text[len(prompt):].strip()\n",
    "\n",
    "    prediction_letter = None\n",
    "    for i, letter in enumerate(OPTION_LETTERS[:len(choices)]):\n",
    "        if respuesta_generada.startswith(f\"{letter}\"):\n",
    "            prediction_letter = letter\n",
    "            break\n",
    "\n",
    "    if prediction_letter in OPTION_LETTERS:\n",
    "        predicted_answer = choices[OPTION_LETTERS.index(prediction_letter)]\n",
    "    else:\n",
    "        predicted_answer = \"[NO DETECTADA]\"\n",
    "\n",
    "    is_correct = predicted_answer == correct_answer\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    predictions.append({\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"choices\": [f\"{OPTION_LETTERS[i]}. {opt}\" for i, opt in enumerate(choices)],\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"generated_text\": generated_text[len(prompt):].strip(),\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"predicted_letter\": prediction_letter,\n",
    "        \"is_correct\": is_correct\n",
    "    })\n",
    "\n",
    "\n",
    "# Métricas\n",
    "end_time = time.time()\n",
    "print(f\"\\nTiempo total: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"\\nPreguntas evaluadas: {total_questions}\")\n",
    "print(f\"Correctas: {correct_predictions}\")\n",
    "print(f\"Precisión: {correct_predictions / total_questions:.2%}\")\n",
    "\n",
    "# Guardar resultados\n",
    "with open(\"2-predictionsLlama1B.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNGYgniH9rIz2yOV/4nrRVE",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
